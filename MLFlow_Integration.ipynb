{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import mlflow\n",
    "random.seed(1024)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in the train, test and validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = './data/train.csv'\n",
    "path_val = './data/validation.csv'\n",
    "path_test = './data/test.csv'\n",
    "path_mod_df = './data/mod_df.csv'\n",
    "\n",
    "train_df = pd.read_csv(path_train)\n",
    "val_df = pd.read_csv(path_val)\n",
    "test_df = pd.read_csv(path_test)\n",
    "raw_data = pd.read_csv(path_mod_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the response from 'spam' and 'ham' to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['y_train'] = train_df['y_train'].map({'ham': 1, 'spam': 0})\n",
    "val_df['y_val'] = val_df['y_val'].map({'ham': 1, 'spam': 0})\n",
    "test_df['y_test'] = test_df['y_test'].map({'ham': 1, 'spam': 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the bag of words transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7331\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_df.X_train)\n",
    "\n",
    "bow_transformer = vectorizer.vocabulary_\n",
    "print(len(bow_transformer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting all the data to be used into bag of words form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4514, 7331) (502, 7331) (558, 7331)\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.transform(train_df.X_train)\n",
    "X_val = vectorizer.transform(val_df.X_val)\n",
    "X_test = vectorizer.transform(test_df.X_test)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the tf-idf transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting the data into tf-idf form**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4514, 7331) (502, 7331) (558, 7331)\n"
     ]
    }
   ],
   "source": [
    "tfidf_X_train = tfidf_transformer.transform(X_train)\n",
    "tfidf_X_val = tfidf_transformer.transform(X_val)\n",
    "tfidf_X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "print(tfidf_X_train.shape, tfidf_X_val.shape, tfidf_X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes Model based on tf-idf tokenizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building a function to create the spam detection model and compute the evaluation metrics for the predicted values based on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNB_model(alp):\n",
    "    spam_detection_model = MultinomialNB(alpha = alp).fit(tfidf_X_train, train_df.y_train)\n",
    "    test_predictions = spam_detection_model.predict(tfidf_X_test)\n",
    "\n",
    "    acc_sc = accuracy_score(test_df.y_test, test_predictions)\n",
    "    \n",
    "    #Computing Precision and Recall\n",
    "    precision, recall, thresholds = precision_recall_curve(test_df.y_test, test_predictions)\n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    aupcr = auc(recall, precision)\n",
    "    \n",
    "    #print(\"The AUPCR score is:\",aupcr)\n",
    "    return [acc_sc, aupcr]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Model based on tf-idf tokenizer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the spam detection model and computing the evaluation metrics for the predicted values based on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Log_model(C):\n",
    "    spam_detection_model_2 = LogisticRegression(C = i)\n",
    "    spam_detection_model_2.fit(tfidf_X_train, train_df.y_train)\n",
    "    test_predictions = spam_detection_model_2.predict(tfidf_X_test)\n",
    "    acc_sc = accuracy_score(test_df.y_test, test_predictions)\n",
    "    \n",
    "    #Computing Precision and Recall\n",
    "    precision, recall, _ = precision_recall_curve(test_df.y_test, test_predictions)\n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    aupcr = auc(recall, precision)\n",
    "    #print(\"The AUPCR score is:\",aupcr)\n",
    "\n",
    "    return [acc_sc, aupcr]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Classifier Model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the spam detection model and computing the evaluation metrics for the predicted values based on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVC_Model(C):    \n",
    "    spam_detection_model_3 = SVC(C = i)\n",
    "    spam_detection_model_3.fit(tfidf_X_train, train_df.y_train)\n",
    "    test_predictions = spam_detection_model_3.predict(tfidf_X_test)\n",
    "    acc_sc = accuracy_score(test_df.y_test, test_predictions)\n",
    "    \n",
    "    #Computing Precision and Recall\n",
    "    precision, recall, _ = precision_recall_curve(test_df.y_test, test_predictions)\n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    aupcr = auc(recall, precision)\n",
    "    #print(\"The AUPCR score is:\",aupcr)\n",
    "\n",
    "    return [acc_sc, aupcr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='MLFlow_Logs\\\\mlruns/362212374075415730', creation_time=1677226009667, experiment_id='362212374075415730', last_update_time=1677226009667, lifecycle_stage='active', name='SMS Spam Classification Model Evaluation', tags={}>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from  mlflow.tracking import MlflowClient\n",
    "client = MlflowClient()\n",
    "#log into MLflow\n",
    "\n",
    "#Set storage directory\n",
    "mlflow.set_tracking_uri('MLFlow_Logs\\mlruns')\n",
    "\n",
    "#set experiment\n",
    "mlflow.set_experiment('SMS Spam Classification Model Evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('SMS Spam Classification Model Evaluation')\n",
    "#Running the models and logging the runs with MLFlow\n",
    "md_name = \"Multinomial Naive Bayes\" \n",
    "for i in np.arange(0.05, 2.25, 0.25):\n",
    "    with mlflow.start_run() as run: #inside brackets run_name='test'\n",
    "        #Log parameters\n",
    "        mlflow.log_param(\"Model\",md_name)\n",
    "        mlflow.log_param(\"Alpha\",i)\n",
    "        #Running the model\n",
    "        model_metrics = MNB_model(i)\n",
    "        #Logging metrics\n",
    "        mlflow.log_metric(\"Accuracy\", model_metrics[0])\n",
    "        mlflow.log_metric(\"AUPCR\", model_metrics[1])\n",
    "\n",
    "md_name = \"Logistic Regression\"\n",
    "for i in [0.1, 0.5, 1, 10, 20, 50, 100]:\n",
    "    with mlflow.start_run() as run: \n",
    "        #Log parameters\n",
    "        mlflow.log_param(\"Model\",md_name)\n",
    "        mlflow.log_param(\"C\",i)\n",
    "        #Running the model\n",
    "        model_metrics = Log_model(i)\n",
    "        #Logging metrics\n",
    "        mlflow.log_metric(\"Accuracy\", model_metrics[0])\n",
    "        mlflow.log_metric(\"AUPCR\", model_metrics[1])\n",
    "\n",
    "\n",
    "md_name = \"Support Vector Classifier\"\n",
    "for i in [0.1, 0.5, 1, 10, 20, 50, 100]:\n",
    "    with mlflow.start_run() as run: \n",
    "        #Log parameters\n",
    "        mlflow.log_param(\"Model\",md_name)\n",
    "        mlflow.log_param(\"Alpha\",i)\n",
    "        #Running the model\n",
    "        model_metrics = MNB_model(i)\n",
    "        #Logging metrics\n",
    "        mlflow.log_metric(\"Accuracy\", model_metrics[0])\n",
    "        mlflow.log_metric(\"AUPCR\", model_metrics[1])\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9839ce3c6acaca560491e1b41f8b46d426659617553cc8a88a5826e3aaa30400"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

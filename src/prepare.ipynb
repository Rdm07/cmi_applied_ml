{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing common packages\n",
    "\n",
    "import os, sys, random\n",
    "import re, string, contractions\n",
    "import nltk, sklearn\n",
    "import csv\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from the file to a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"../data/SMSSpamCollection\")\n",
    "sms_data_list = data_file.readlines()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the list into a list of [label, data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n"
     ]
    }
   ],
   "source": [
    "sms_labelled_data = [[i.split(\"\\t\")[0], i.split(\"\\t\")[1].split(\"\\n\")[0]] for i in sms_data_list]\n",
    "print(sms_labelled_data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to get words from the sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sent):\n",
    "    sent = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sent, flags=re.MULTILINE) # Remove urls starting with http\n",
    "    sent = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', sent, flags=re.MULTILINE) # Remove urls starting with https\n",
    "    sent = contractions.fix(sent, slang=True) # Replace contractions with words\n",
    "    sent = ''.join([i for i in sent if not i.isdigit()]) # Remove numbers\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_words = tokenizer.tokenize(sent) # Remove all punctuation marks (don't have to worry about contractions)\n",
    "    return tokenized_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to return a vocabulary (with number of occurences) upon given an sms as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sms(sms):\n",
    "    words_list = get_words(sms)\n",
    "    \n",
    "    stopwords = list(nltk.corpus.stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    temp_list = []\n",
    "\n",
    "    for word in words_list:\n",
    "        if len(word) > 1 and word.lower() not in stopwords:\n",
    "            word = lemmatizer.lemmatize(word.lower())\n",
    "            temp_list.append(word.lower())\n",
    "\n",
    "    return temp_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the defined functions to convert the labelled list of sms into a labelled list of vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ham',\n",
       " ['go',\n",
       "  'jurong',\n",
       "  'point',\n",
       "  'crazy',\n",
       "  'available',\n",
       "  'bugis',\n",
       "  'great',\n",
       "  'world',\n",
       "  'la',\n",
       "  'buffet',\n",
       "  'cine',\n",
       "  'got',\n",
       "  'amore',\n",
       "  'wat'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_labels = []\n",
    "sms_words = []\n",
    "\n",
    "for item in sms_labelled_data:\n",
    "    word_list = get_tokenized_sms(item[1])\n",
    "\n",
    "    sms_labels.append(item[0])\n",
    "    sms_words.append(word_list)\n",
    "\n",
    "sms_labels[0], sms_words[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to save the labels and words as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(filename, x_data, y_data):\n",
    "    combined_data = []\n",
    "    \n",
    "    for i in range(len(y_data)):\n",
    "        temp = []\n",
    "        temp.append(y_data[i])\n",
    "        temp = temp + x_data[i]\n",
    "        combined_data.append(temp)\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerows(combined_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining all the above preprocessing steps into one function to save a raw_data.csv file from the given text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_from_txt(file_path):\n",
    "    data_file = open(file_path)\n",
    "    sms_data_list = data_file.readlines()\n",
    "    sms_labelled_data = [[i.split(\"\\t\")[0], i.split(\"\\t\")[1].split(\"\\n\")[0]] for i in sms_data_list]\n",
    "\n",
    "    sms_labels = []\n",
    "    sms_words = []\n",
    "\n",
    "    for item in sms_labelled_data:\n",
    "        word_list = get_tokenized_sms(item[1])\n",
    "\n",
    "        sms_labels.append(item[0])\n",
    "        sms_words.append(word_list)\n",
    "\n",
    "    save_csv(\"../data/raw_data.csv\", sms_words, sms_labels)\n",
    "\n",
    "    return sms_labels, sms_words\n",
    "\n",
    "sms_labels, sms_words = get_csv_from_txt(\"../data/SMSSpamCollection\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the raw_data into lists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to load data from csv to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_from_csv(file_path):\n",
    "    with open(file_path, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        sms_words = list(reader)\n",
    "\n",
    "    sms_labels = [x[0] for x in sms_words]\n",
    "\n",
    "    for x in sms_words:\n",
    "        del x[0]\n",
    "\n",
    "    return sms_labels, sms_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into train/validation/test datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to split data into train/val/test sets and saving as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data_x, data_y, val_per, test_per, tr_path, val_path, te_path, random_seed):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=test_per, shuffle=True, random_state=random_seed)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_per/(1-test_per), shuffle=True, random_state=random_seed)\n",
    "\n",
    "    save_csv(tr_path, x_train, y_train)\n",
    "    save_csv(val_path, x_val, y_val)\n",
    "    save_csv(te_path, x_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining file paths for train/val/test csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/train_data.csv\"\n",
    "val_path = \"../data/val_data.csv\"\n",
    "test_path = \"../data/test_data.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data using random seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_split(sms_words, sms_labels, 0.15, 0.15, train_path, val_path, test_path, 42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the split csv using dvc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n",
      "\n",
      "You can now commit the changes to git.\n",
      "\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m        DVC has enabled anonymous aggregate usage analytics.         \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m     Read the analytics documentation (and how to opt-out) here:     \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m             <\u001b[36mhttps://dvc.org/doc/user-guide/analytics\u001b[39m>              \u001b[31m|\u001b[0m\n",
      "\u001b[31m|\u001b[0m                                                                     \u001b[31m|\u001b[0m\n",
      "\u001b[31m+---------------------------------------------------------------------+\n",
      "\u001b[0m\n",
      "\u001b[33mWhat's next?\u001b[39m\n",
      "\u001b[33m------------\u001b[39m\n",
      "- Check out the documentation: <\u001b[36mhttps://dvc.org/doc\u001b[39m>\n",
      "- Get help and share ideas: <\u001b[36mhttps://dvc.org/chat\u001b[39m>\n",
      "- Star us on GitHub: <\u001b[36mhttps://github.com/iterative/dvc\u001b[39m>\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!cd .. && dvc init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all three csv files to dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[32m⠋\u001b[0m Checking graph                                                   \u001b[32m⠋\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "\u001b[31mERROR\u001b[39m:  output '../data/train_data.csv' is already tracked by SCM (e.g. Git).\n",
      "    You can remove it from Git, then add to DVC.\n",
      "        To stop tracking from Git:\n",
      "            git rm -r --cached '../data/train_data.csv'\n",
      "            git commit -m \"stop tracking ../data/train_data.csv\" \n",
      "\u001b[?25l                                                                          \u001b[32m⠋\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "\u001b[31mERROR\u001b[39m:  output '../data/val_data.csv' is already tracked by SCM (e.g. Git).\n",
      "    You can remove it from Git, then add to DVC.\n",
      "        To stop tracking from Git:\n",
      "            git rm -r --cached '../data/val_data.csv'\n",
      "            git commit -m \"stop tracking ../data/val_data.csv\" \n",
      "\u001b[?25l                                                                          \u001b[32m⠋\u001b[0m Checking graph\n",
      "Adding...                                                                       \n",
      "\u001b[31mERROR\u001b[39m:  output '../data/test_data.csv' is already tracked by SCM (e.g. Git).\n",
      "    You can remove it from Git, then add to DVC.\n",
      "        To stop tracking from Git:\n",
      "            git rm -r --cached '../data/test_data.csv'\n",
      "            git commit -m \"stop tracking ../data/test_data.csv\" \n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc add ../data/train_data.csv\n",
    "!dvc add ../data/val_data.csv\n",
    "!dvc add ../data/test_data.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding google drive folder as a remote data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd .. && dvc remote add --default myremote gdrive://1MypipdcBtjmYnO3OQQmLxKM3SWfwmE2p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cmi_all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb51b6b13e8232a64b369b5696b9d41218df7738edf9356f9fbcee5c06010241"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

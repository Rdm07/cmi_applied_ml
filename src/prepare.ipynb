{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing common packages\n",
    "\n",
    "import os, sys, random\n",
    "import re, string, contractions\n",
    "import nltk, sklearn\n",
    "import csv\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from the file to a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open(\"../data/SMSSpamCollection\")\n",
    "sms_data_list = data_file.readlines()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the list into a list of [label, data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n"
     ]
    }
   ],
   "source": [
    "sms_labelled_data = [[i.split(\"\\t\")[0], i.split(\"\\t\")[1].split(\"\\n\")[0]] for i in sms_data_list]\n",
    "print(sms_labelled_data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to get words from the sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(sent):\n",
    "    sent = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', sent, flags=re.MULTILINE) # Remove urls starting with http\n",
    "    sent = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', sent, flags=re.MULTILINE) # Remove urls starting with https\n",
    "    sent = contractions.fix(sent, slang=True) # Replace contractions with words\n",
    "    sent = ''.join([i for i in sent if not i.isdigit()]) # Remove numbers\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_words = tokenizer.tokenize(sent) # Remove all punctuation marks (don't have to worry about contractions)\n",
    "    return tokenized_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to return a vocabulary (with number of occurences) upon given an sms as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sms(sms):\n",
    "    words_list = get_words(sms)\n",
    "    \n",
    "    stopwords = list(nltk.corpus.stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    temp_list = []\n",
    "\n",
    "    for word in words_list:\n",
    "        if len(word) > 1 and word.lower() not in stopwords:\n",
    "            word = lemmatizer.lemmatize(word.lower())\n",
    "            temp_list.append(word.lower())\n",
    "\n",
    "    return temp_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the defined functions to convert the labelled list of sms into a labelled list of vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ham',\n",
       " ['go',\n",
       "  'jurong',\n",
       "  'point',\n",
       "  'crazy',\n",
       "  'available',\n",
       "  'bugis',\n",
       "  'great',\n",
       "  'world',\n",
       "  'la',\n",
       "  'buffet',\n",
       "  'cine',\n",
       "  'got',\n",
       "  'amore',\n",
       "  'wat'])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_labels = []\n",
    "sms_words = []\n",
    "\n",
    "for item in sms_labelled_data:\n",
    "    word_list = get_tokenized_sms(item[1])\n",
    "\n",
    "    sms_labels.append(item[0])\n",
    "    sms_words.append(word_list)\n",
    "\n",
    "sms_labels[0], sms_words[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train/validation/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3901 836 837\n"
     ]
    }
   ],
   "source": [
    "test_per = 0.15\n",
    "val_per = 0.15\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(sms_words, sms_labels, test_size=test_per, shuffle=True)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_per/(1-test_per), shuffle=True)\n",
    "\n",
    "print(len(x_train), len(x_val), len(x_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving each dataset as a csv file with the first word on each line being the label for that datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(filename, x_data, y_data):\n",
    "    combined_data = []\n",
    "    \n",
    "    for i in range(len(y_data)):\n",
    "        temp = []\n",
    "        temp.append(y_data[i])\n",
    "        temp = temp + x_data[i]\n",
    "        combined_data.append(temp)\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        write = csv.writer(f)\n",
    "        write.writerows(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(\"../data/train_data.csv\", x_train, y_train)\n",
    "save_csv(\"../data/val_data.csv\", x_val, y_val)\n",
    "save_csv(\"../data/test_data.csv\", x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cmi_all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb51b6b13e8232a64b369b5696b9d41218df7738edf9356f9fbcee5c06010241"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
